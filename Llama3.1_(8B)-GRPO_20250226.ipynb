{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Freakz3z/DeepSeek-Recipie-Food/blob/main/Llama3.1_(8B)-GRPO_20250226.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0B0ZE99AwoW"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8GpT4ZvAwoX"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOY-7M_kAwoX"
      },
      "source": [
        "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "下载CUDA环境（仅本地运行需要）"
      ],
      "metadata": {
        "id": "6--7YtnA5xhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "29uUjjDe2LvX",
        "outputId": "653a66db-9959-4be8-c7ff-b8542b661b11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (2.6.0+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (0.21.0)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (2.6.0+cu118)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "检查CUDA环境"
      ],
      "metadata": {
        "id": "e7C5WCgj6sP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 检查 CUDA 是否可用\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"CUDA not available. Please check your installation.\")"
      ],
      "metadata": {
        "id": "K6cprFX36sEs",
        "outputId": "796c1c06-7fc4-4a7c-95c8-db3bbd192cf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using GPU.\n",
            "Number of GPUs: 1\n",
            "GPU Name: NVIDIA GeForce RTX 4090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkTtU_qeAwoX"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYPqjiiIAwoY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Skip restarting message in Colab\n",
        "import sys; modules = list(sys.modules.keys())\n",
        "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "\n",
        "!pip install unsloth\n",
        "!pip install vllm\n",
        "!pip install --upgrade pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "865ggYfLKXou"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm"
      ],
      "metadata": {
        "id": "-mzzO1njQngs",
        "outputId": "24176fc3-8e50-45ba-f937-d403c538773e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.7.3.tar.gz (5.6 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (5.9.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (1.26.4)\n",
            "Requirement already satisfied: numba==0.60.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (0.60.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (4.66.5)\n",
            "Collecting blake3 (from vllm)\n",
            "  Using cached blake3-1.0.4-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: py-cpuinfo in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.48.2 in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (from vllm) (4.49.0)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (from vllm) (0.21.0)\n",
            "Requirement already satisfied: protobuf in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (from vllm) (3.20.3)\n",
            "Collecting fastapi!=0.113.*,!=0.114.0,>=0.107.0 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (3.10.5)\n",
            "Collecting openai>=1.52.0 (from vllm)\n",
            "  Using cached openai-1.64.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting pydantic>=2.9 (from vllm)\n",
            "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting prometheus_client>=0.18.0 (from vllm)\n",
            "  Using cached prometheus_client-0.21.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pillow in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (from vllm) (11.1.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
            "  Using cached prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken>=0.6.0 (from vllm)\n",
            "  Using cached tiktoken-0.9.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.10-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting outlines==0.1.11 (from vllm)\n",
            "  Using cached outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lark==1.2.2 (from vllm)\n",
            "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (4.11.0)\n",
            "Collecting filelock>=3.16.1 (from vllm)\n",
            "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting partial-json-parser (from vllm)\n",
            "  Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (25.1.2)\n",
            "Collecting msgspec (from vllm)\n",
            "  Using cached msgspec-0.19.0-cp312-cp312-win_amd64.whl.metadata (7.1 kB)\n",
            "Collecting gguf==0.10.0 (from vllm)\n",
            "  Using cached gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: importlib_metadata in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (7.0.1)\n",
            "Collecting mistral_common>=1.5.0 (from mistral_common[opencv]>=1.5.0->vllm)\n",
            "  Using cached mistral_common-1.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (6.0.1)\n",
            "Requirement already satisfied: six>=1.16.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (1.16.0)\n",
            "Requirement already satisfied: setuptools>=74.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (75.1.0)\n",
            "Collecting einops (from vllm)\n",
            "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting compressed-tensors==0.9.1 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting depyf==0.18.0 (from vllm)\n",
            "  Using cached depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from vllm) (3.0.0)\n",
            "Requirement already satisfied: torch>=1.7.0 in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (from compressed-tensors==0.9.1->vllm) (2.6.0+cu118)\n",
            "Collecting astor (from depyf==0.18.0->vllm)\n",
            "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in c:\\programdata\\anaconda3\\lib\\site-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba==0.60.0->vllm) (0.43.0)\n",
            "Collecting interegular (from outlines==0.1.11->vllm)\n",
            "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from outlines==0.1.11->vllm) (3.1.4)\n",
            "Requirement already satisfied: nest_asyncio in c:\\programdata\\anaconda3\\lib\\site-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Collecting diskcache (from outlines==0.1.11->vllm)\n",
            "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: referencing in c:\\programdata\\anaconda3\\lib\\site-packages (from outlines==0.1.11->vllm) (0.30.2)\n",
            "Requirement already satisfied: jsonschema in c:\\programdata\\anaconda3\\lib\\site-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Collecting pycountry (from outlines==0.1.11->vllm)\n",
            "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
            "  Using cached airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
            "  Using cached outlines_core-0.1.26-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (0.27.0)\n",
            "Collecting jinja2 (from outlines==0.1.11->vllm)\n",
            "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting python-multipart>=0.0.18 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm) (24.1)\n",
            "Collecting opencv-python-headless>=4.0.0 (from mistral_common[opencv]>=1.5.0->vllm)\n",
            "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai>=1.52.0->vllm) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.52.0->vllm)\n",
            "  Using cached jiter-0.8.2-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai>=1.52.0->vllm) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic>=2.9->vllm) (0.6.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic>=2.9->vllm)\n",
            "  Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
            "Collecting typing_extensions>=4.10 (from vllm)\n",
            "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->vllm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->vllm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->vllm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->vllm) (2024.12.14)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken>=0.6.0->vllm) (2024.9.11)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (from tokenizers>=0.19.1->vllm) (0.29.1)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->vllm) (0.4.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (from transformers>=4.48.2->vllm) (0.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->vllm) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->vllm) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->vllm) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->vllm) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->vllm) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->vllm) (1.11.0)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib_metadata->vllm) (3.17.0)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached rich_toolkit-0.13.2-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (0.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.1->vllm) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->outlines==0.1.11->vllm) (2.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema->outlines==0.1.11->vllm) (2023.7.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema->outlines==0.1.11->vllm) (0.10.6)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.7.0->compressed-tensors==0.9.1->vllm) (3.3)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\test\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.7.0->compressed-tensors==0.9.1->vllm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.7.0->compressed-tensors==0.9.1->vllm) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (8.1.7)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (0.21.0)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached watchfiles-1.0.4-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm)\n",
            "  Using cached websockets-15.0-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: rich>=13.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (13.7.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (1.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm) (0.1.0)\n",
            "Using cached compressed_tensors-0.9.1-py3-none-any.whl (96 kB)\n",
            "Using cached depyf-0.18.0-py3-none-any.whl (38 kB)\n",
            "Using cached gguf-0.10.0-py3-none-any.whl (71 kB)\n",
            "Using cached lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "Using cached outlines-0.1.11-py3-none-any.whl (87 kB)\n",
            "Using cached outlines_core-0.1.26-cp312-cp312-win_amd64.whl (243 kB)\n",
            "Using cached fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
            "Using cached lm_format_enforcer-0.10.10-py3-none-any.whl (44 kB)\n",
            "Using cached mistral_common-1.5.3-py3-none-any.whl (6.5 MB)\n",
            "Using cached openai-1.64.0-py3-none-any.whl (472 kB)\n",
            "Using cached prometheus_client-0.21.1-py3-none-any.whl (54 kB)\n",
            "Using cached prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl (18 kB)\n",
            "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
            "Using cached tiktoken-0.9.0-cp312-cp312-win_amd64.whl (894 kB)\n",
            "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Using cached blake3-1.0.4-cp312-cp312-win_amd64.whl (212 kB)\n",
            "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Using cached msgspec-0.19.0-cp312-cp312-win_amd64.whl (187 kB)\n",
            "Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
            "Using cached email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Using cached fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Using cached interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
            "Using cached jiter-0.8.2-cp312-cp312-win_amd64.whl (204 kB)\n",
            "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
            "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Using cached starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "Using cached airportsdata-20250224-py3-none-any.whl (913 kB)\n",
            "Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "Using cached pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "Using cached dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl (88 kB)\n",
            "Using cached rich_toolkit-0.13.2-py3-none-any.whl (13 kB)\n",
            "Using cached typer-0.15.1-py3-none-any.whl (44 kB)\n",
            "Using cached watchfiles-1.0.4-cp312-cp312-win_amd64.whl (285 kB)\n",
            "Using cached websockets-15.0-cp312-cp312-win_amd64.whl (176 kB)\n",
            "Building wheels for collected packages: vllm\n",
            "  Building wheel for vllm (pyproject.toml): started\n",
            "  Building wheel for vllm (pyproject.toml): finished with status 'error'\n",
            "Failed to build vllm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  Building wheel for vllm (pyproject.toml) did not run successfully.\n",
            "  exit code: 1\n",
            "  \n",
            "  [1466 lines of output]\n",
            "  C:\\Users\\Test\\AppData\\Local\\Temp\\pip-build-env-wdoo4r3e\\overlay\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
            "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
            "  vLLM only supports Linux platform (including WSL) and MacOS.Building on win32, so vLLM may not be able to run correctly\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\\lib\\vllm\n",
            "  copying vllm\\beam_search.py -> build\\lib\\vllm\n",
            "  copying vllm\\config.py -> build\\lib\\vllm\n",
            "  copying vllm\\connections.py -> build\\lib\\vllm\n",
            "  copying vllm\\envs.py -> build\\lib\\vllm\n",
            "  copying vllm\\forward_context.py -> build\\lib\\vllm\n",
            "  copying vllm\\logger.py -> build\\lib\\vllm\n",
            "  copying vllm\\logits_process.py -> build\\lib\\vllm\n",
            "  copying vllm\\outputs.py -> build\\lib\\vllm\n",
            "  copying vllm\\pooling_params.py -> build\\lib\\vllm\n",
            "  copying vllm\\sampling_params.py -> build\\lib\\vllm\n",
            "  copying vllm\\scalar_type.py -> build\\lib\\vllm\n",
            "  copying vllm\\scripts.py -> build\\lib\\vllm\n",
            "  copying vllm\\sequence.py -> build\\lib\\vllm\n",
            "  copying vllm\\tracing.py -> build\\lib\\vllm\n",
            "  copying vllm\\utils.py -> build\\lib\\vllm\n",
            "  copying vllm\\version.py -> build\\lib\\vllm\n",
            "  copying vllm\\_custom_ops.py -> build\\lib\\vllm\n",
            "  copying vllm\\_ipex_ops.py -> build\\lib\\vllm\n",
            "  copying vllm\\_version.py -> build\\lib\\vllm\n",
            "  copying vllm\\__init__.py -> build\\lib\\vllm\n",
            "  creating build\\lib\\vllm\\adapter_commons\n",
            "  copying vllm\\adapter_commons\\layers.py -> build\\lib\\vllm\\adapter_commons\n",
            "  copying vllm\\adapter_commons\\models.py -> build\\lib\\vllm\\adapter_commons\n",
            "  copying vllm\\adapter_commons\\request.py -> build\\lib\\vllm\\adapter_commons\n",
            "  copying vllm\\adapter_commons\\utils.py -> build\\lib\\vllm\\adapter_commons\n",
            "  copying vllm\\adapter_commons\\worker_manager.py -> build\\lib\\vllm\\adapter_commons\n",
            "  copying vllm\\adapter_commons\\__init__.py -> build\\lib\\vllm\\adapter_commons\n",
            "  creating build\\lib\\vllm\\assets\n",
            "  copying vllm\\assets\\audio.py -> build\\lib\\vllm\\assets\n",
            "  copying vllm\\assets\\base.py -> build\\lib\\vllm\\assets\n",
            "  copying vllm\\assets\\image.py -> build\\lib\\vllm\\assets\n",
            "  copying vllm\\assets\\video.py -> build\\lib\\vllm\\assets\n",
            "  copying vllm\\assets\\__init__.py -> build\\lib\\vllm\\assets\n",
            "  creating build\\lib\\vllm\\attention\n",
            "  copying vllm\\attention\\layer.py -> build\\lib\\vllm\\attention\n",
            "  copying vllm\\attention\\selector.py -> build\\lib\\vllm\\attention\n",
            "  copying vllm\\attention\\__init__.py -> build\\lib\\vllm\\attention\n",
            "  creating build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\backends.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\compiler_interface.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\counter.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\decorators.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\fix_functionalization.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\fusion.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\fx_utils.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\inductor_pass.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\monitor.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\multi_output_match.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\pass_manager.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\reshapes.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\vllm_inductor_pass.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\wrapper.py -> build\\lib\\vllm\\compilation\n",
            "  copying vllm\\compilation\\__init__.py -> build\\lib\\vllm\\compilation\n",
            "  creating build\\lib\\vllm\\core\n",
            "  copying vllm\\core\\block_manager.py -> build\\lib\\vllm\\core\n",
            "  copying vllm\\core\\evictor.py -> build\\lib\\vllm\\core\n",
            "  copying vllm\\core\\interfaces.py -> build\\lib\\vllm\\core\n",
            "  copying vllm\\core\\placeholder_block_space_manager.py -> build\\lib\\vllm\\core\n",
            "  copying vllm\\core\\scheduler.py -> build\\lib\\vllm\\core\n",
            "  copying vllm\\core\\__init__.py -> build\\lib\\vllm\\core\n",
            "  creating build\\lib\\vllm\\device_allocator\n",
            "  copying vllm\\device_allocator\\cumem.py -> build\\lib\\vllm\\device_allocator\n",
            "  copying vllm\\device_allocator\\__init__.py -> build\\lib\\vllm\\device_allocator\n",
            "  creating build\\lib\\vllm\\distributed\n",
            "  copying vllm\\distributed\\communication_op.py -> build\\lib\\vllm\\distributed\n",
            "  copying vllm\\distributed\\parallel_state.py -> build\\lib\\vllm\\distributed\n",
            "  copying vllm\\distributed\\utils.py -> build\\lib\\vllm\\distributed\n",
            "  copying vllm\\distributed\\__init__.py -> build\\lib\\vllm\\distributed\n",
            "  creating build\\lib\\vllm\\engine\n",
            "  copying vllm\\engine\\arg_utils.py -> build\\lib\\vllm\\engine\n",
            "  copying vllm\\engine\\async_llm_engine.py -> build\\lib\\vllm\\engine\n",
            "  copying vllm\\engine\\async_timeout.py -> build\\lib\\vllm\\engine\n",
            "  copying vllm\\engine\\llm_engine.py -> build\\lib\\vllm\\engine\n",
            "  copying vllm\\engine\\metrics.py -> build\\lib\\vllm\\engine\n",
            "  copying vllm\\engine\\metrics_types.py -> build\\lib\\vllm\\engine\n",
            "  copying vllm\\engine\\protocol.py -> build\\lib\\vllm\\engine\n",
            "  copying vllm\\engine\\__init__.py -> build\\lib\\vllm\\engine\n",
            "  creating build\\lib\\vllm\\entrypoints\n",
            "  copying vllm\\entrypoints\\api_server.py -> build\\lib\\vllm\\entrypoints\n",
            "  copying vllm\\entrypoints\\chat_utils.py -> build\\lib\\vllm\\entrypoints\n",
            "  copying vllm\\entrypoints\\launcher.py -> build\\lib\\vllm\\entrypoints\n",
            "  copying vllm\\entrypoints\\llm.py -> build\\lib\\vllm\\entrypoints\n",
            "  copying vllm\\entrypoints\\logger.py -> build\\lib\\vllm\\entrypoints\n",
            "  copying vllm\\entrypoints\\utils.py -> build\\lib\\vllm\\entrypoints\n",
            "  copying vllm\\entrypoints\\__init__.py -> build\\lib\\vllm\\entrypoints\n",
            "  creating build\\lib\\vllm\\executor\n",
            "  copying vllm\\executor\\executor_base.py -> build\\lib\\vllm\\executor\n",
            "  copying vllm\\executor\\mp_distributed_executor.py -> build\\lib\\vllm\\executor\n",
            "  copying vllm\\executor\\msgspec_utils.py -> build\\lib\\vllm\\executor\n",
            "  copying vllm\\executor\\multiproc_worker_utils.py -> build\\lib\\vllm\\executor\n",
            "  copying vllm\\executor\\ray_distributed_executor.py -> build\\lib\\vllm\\executor\n",
            "  copying vllm\\executor\\ray_utils.py -> build\\lib\\vllm\\executor\n",
            "  copying vllm\\executor\\uniproc_executor.py -> build\\lib\\vllm\\executor\n",
            "  copying vllm\\executor\\__init__.py -> build\\lib\\vllm\\executor\n",
            "  creating build\\lib\\vllm\\inputs\n",
            "  copying vllm\\inputs\\data.py -> build\\lib\\vllm\\inputs\n",
            "  copying vllm\\inputs\\parse.py -> build\\lib\\vllm\\inputs\n",
            "  copying vllm\\inputs\\preprocess.py -> build\\lib\\vllm\\inputs\n",
            "  copying vllm\\inputs\\registry.py -> build\\lib\\vllm\\inputs\n",
            "  copying vllm\\inputs\\__init__.py -> build\\lib\\vllm\\inputs\n",
            "  creating build\\lib\\vllm\\logging_utils\n",
            "  copying vllm\\logging_utils\\formatter.py -> build\\lib\\vllm\\logging_utils\n",
            "  copying vllm\\logging_utils\\__init__.py -> build\\lib\\vllm\\logging_utils\n",
            "  creating build\\lib\\vllm\\lora\n",
            "  copying vllm\\lora\\fully_sharded_layers.py -> build\\lib\\vllm\\lora\n",
            "  copying vllm\\lora\\layers.py -> build\\lib\\vllm\\lora\n",
            "  copying vllm\\lora\\lora.py -> build\\lib\\vllm\\lora\n",
            "  copying vllm\\lora\\models.py -> build\\lib\\vllm\\lora\n",
            "  copying vllm\\lora\\peft_helper.py -> build\\lib\\vllm\\lora\n",
            "  copying vllm\\lora\\request.py -> build\\lib\\vllm\\lora\n",
            "  copying vllm\\lora\\utils.py -> build\\lib\\vllm\\lora\n",
            "  copying vllm\\lora\\worker_manager.py -> build\\lib\\vllm\\lora\n",
            "  copying vllm\\lora\\__init__.py -> build\\lib\\vllm\\lora\n",
            "  creating build\\lib\\vllm\\model_executor\n",
            "  copying vllm\\model_executor\\custom_op.py -> build\\lib\\vllm\\model_executor\n",
            "  copying vllm\\model_executor\\parameter.py -> build\\lib\\vllm\\model_executor\n",
            "  copying vllm\\model_executor\\pooling_metadata.py -> build\\lib\\vllm\\model_executor\n",
            "  copying vllm\\model_executor\\sampling_metadata.py -> build\\lib\\vllm\\model_executor\n",
            "  copying vllm\\model_executor\\utils.py -> build\\lib\\vllm\\model_executor\n",
            "  copying vllm\\model_executor\\__init__.py -> build\\lib\\vllm\\model_executor\n",
            "  creating build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\audio.py -> build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\base.py -> build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\hasher.py -> build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\image.py -> build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\inputs.py -> build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\parse.py -> build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\processing.py -> build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\profiling.py -> build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\registry.py -> build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\utils.py -> build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\video.py -> build\\lib\\vllm\\multimodal\n",
            "  copying vllm\\multimodal\\__init__.py -> build\\lib\\vllm\\multimodal\n",
            "  creating build\\lib\\vllm\\platforms\n",
            "  copying vllm\\platforms\\cpu.py -> build\\lib\\vllm\\platforms\n",
            "  copying vllm\\platforms\\cuda.py -> build\\lib\\vllm\\platforms\n",
            "  copying vllm\\platforms\\hpu.py -> build\\lib\\vllm\\platforms\n",
            "  copying vllm\\platforms\\interface.py -> build\\lib\\vllm\\platforms\n",
            "  copying vllm\\platforms\\neuron.py -> build\\lib\\vllm\\platforms\n",
            "  copying vllm\\platforms\\openvino.py -> build\\lib\\vllm\\platforms\n",
            "  copying vllm\\platforms\\rocm.py -> build\\lib\\vllm\\platforms\n",
            "  copying vllm\\platforms\\tpu.py -> build\\lib\\vllm\\platforms\n",
            "  copying vllm\\platforms\\xpu.py -> build\\lib\\vllm\\platforms\n",
            "  copying vllm\\platforms\\__init__.py -> build\\lib\\vllm\\platforms\n",
            "  creating build\\lib\\vllm\\plugins\n",
            "  copying vllm\\plugins\\__init__.py -> build\\lib\\vllm\\plugins\n",
            "  creating build\\lib\\vllm\\profiler\n",
            "  copying vllm\\profiler\\layerwise_profile.py -> build\\lib\\vllm\\profiler\n",
            "  copying vllm\\profiler\\utils.py -> build\\lib\\vllm\\profiler\n",
            "  copying vllm\\profiler\\__init__.py -> build\\lib\\vllm\\profiler\n",
            "  creating build\\lib\\vllm\\prompt_adapter\n",
            "  copying vllm\\prompt_adapter\\layers.py -> build\\lib\\vllm\\prompt_adapter\n",
            "  copying vllm\\prompt_adapter\\models.py -> build\\lib\\vllm\\prompt_adapter\n",
            "  copying vllm\\prompt_adapter\\request.py -> build\\lib\\vllm\\prompt_adapter\n",
            "  copying vllm\\prompt_adapter\\utils.py -> build\\lib\\vllm\\prompt_adapter\n",
            "  copying vllm\\prompt_adapter\\worker_manager.py -> build\\lib\\vllm\\prompt_adapter\n",
            "  copying vllm\\prompt_adapter\\__init__.py -> build\\lib\\vllm\\prompt_adapter\n",
            "  creating build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\batch_expansion.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\draft_model_runner.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\interfaces.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\medusa_worker.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\metrics.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\mlp_speculator_worker.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\mqa_scorer.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\multi_step_worker.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\ngram_worker.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\proposer_worker_base.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\smaller_tp_proposer_worker.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\spec_decode_worker.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\target_model_runner.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\top1_proposer.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\util.py -> build\\lib\\vllm\\spec_decode\n",
            "  copying vllm\\spec_decode\\__init__.py -> build\\lib\\vllm\\spec_decode\n",
            "  creating build\\lib\\vllm\\third_party\n",
            "  copying vllm\\third_party\\pynvml.py -> build\\lib\\vllm\\third_party\n",
            "  copying vllm\\third_party\\__init__.py -> build\\lib\\vllm\\third_party\n",
            "  creating build\\lib\\vllm\\transformers_utils\n",
            "  copying vllm\\transformers_utils\\config.py -> build\\lib\\vllm\\transformers_utils\n",
            "  copying vllm\\transformers_utils\\detokenizer.py -> build\\lib\\vllm\\transformers_utils\n",
            "  copying vllm\\transformers_utils\\detokenizer_utils.py -> build\\lib\\vllm\\transformers_utils\n",
            "  copying vllm\\transformers_utils\\processor.py -> build\\lib\\vllm\\transformers_utils\n",
            "  copying vllm\\transformers_utils\\s3_utils.py -> build\\lib\\vllm\\transformers_utils\n",
            "  copying vllm\\transformers_utils\\tokenizer.py -> build\\lib\\vllm\\transformers_utils\n",
            "  copying vllm\\transformers_utils\\tokenizer_base.py -> build\\lib\\vllm\\transformers_utils\n",
            "  copying vllm\\transformers_utils\\utils.py -> build\\lib\\vllm\\transformers_utils\n",
            "  copying vllm\\transformers_utils\\__init__.py -> build\\lib\\vllm\\transformers_utils\n",
            "  creating build\\lib\\vllm\\triton_utils\n",
            "  copying vllm\\triton_utils\\custom_cache_manager.py -> build\\lib\\vllm\\triton_utils\n",
            "  copying vllm\\triton_utils\\importing.py -> build\\lib\\vllm\\triton_utils\n",
            "  copying vllm\\triton_utils\\__init__.py -> build\\lib\\vllm\\triton_utils\n",
            "  creating build\\lib\\vllm\\usage\n",
            "  copying vllm\\usage\\usage_lib.py -> build\\lib\\vllm\\usage\n",
            "  copying vllm\\usage\\__init__.py -> build\\lib\\vllm\\usage\n",
            "  creating build\\lib\\vllm\\v1\n",
            "  copying vllm\\v1\\kv_cache_interface.py -> build\\lib\\vllm\\v1\n",
            "  copying vllm\\v1\\outputs.py -> build\\lib\\vllm\\v1\n",
            "  copying vllm\\v1\\request.py -> build\\lib\\vllm\\v1\n",
            "  copying vllm\\v1\\serial_utils.py -> build\\lib\\vllm\\v1\n",
            "  copying vllm\\v1\\utils.py -> build\\lib\\vllm\\v1\n",
            "  copying vllm\\v1\\__init__.py -> build\\lib\\vllm\\v1\n",
            "  creating build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\cache_engine.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\cpu_enc_dec_model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\cpu_model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\cpu_pooling_model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\cpu_worker.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\enc_dec_model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\hpu_model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\hpu_worker.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\model_runner_base.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\multi_step_model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\multi_step_tpu_worker.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\multi_step_worker.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\neuron_model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\neuron_worker.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\openvino_model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\openvino_worker.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\pooling_model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\tpu_model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\tpu_worker.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\utils.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\worker.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\worker_base.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\xpu_model_runner.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\xpu_worker.py -> build\\lib\\vllm\\worker\n",
            "  copying vllm\\worker\\__init__.py -> build\\lib\\vllm\\worker\n",
            "  creating build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\abstract.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\blocksparse_attn.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\flashinfer.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\flash_attn.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\hpu_attn.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\ipex_attn.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\openvino.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\pallas.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\placeholder_attn.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\rocm_flash_attn.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\torch_sdpa.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\triton_mla.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\utils.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\xformers.py -> build\\lib\\vllm\\attention\\backends\n",
            "  copying vllm\\attention\\backends\\__init__.py -> build\\lib\\vllm\\attention\\backends\n",
            "  creating build\\lib\\vllm\\attention\\ops\n",
            "  copying vllm\\attention\\ops\\hpu_paged_attn.py -> build\\lib\\vllm\\attention\\ops\n",
            "  copying vllm\\attention\\ops\\ipex_attn.py -> build\\lib\\vllm\\attention\\ops\n",
            "  copying vllm\\attention\\ops\\nki_flash_attn.py -> build\\lib\\vllm\\attention\\ops\n",
            "  copying vllm\\attention\\ops\\paged_attn.py -> build\\lib\\vllm\\attention\\ops\n",
            "  copying vllm\\attention\\ops\\prefix_prefill.py -> build\\lib\\vllm\\attention\\ops\n",
            "  copying vllm\\attention\\ops\\triton_decode_attention.py -> build\\lib\\vllm\\attention\\ops\n",
            "  copying vllm\\attention\\ops\\triton_flash_attention.py -> build\\lib\\vllm\\attention\\ops\n",
            "  copying vllm\\attention\\ops\\__init__.py -> build\\lib\\vllm\\attention\\ops\n",
            "  creating build\\lib\\vllm\\attention\\backends\\mla\n",
            "  copying vllm\\attention\\backends\\mla\\utils.py -> build\\lib\\vllm\\attention\\backends\\mla\n",
            "  copying vllm\\attention\\backends\\mla\\__init__.py -> build\\lib\\vllm\\attention\\backends\\mla\n",
            "  creating build\\lib\\vllm\\attention\\ops\\blocksparse_attention\n",
            "  copying vllm\\attention\\ops\\blocksparse_attention\\blocksparse_attention_kernel.py -> build\\lib\\vllm\\attention\\ops\\blocksparse_attention\n",
            "  copying vllm\\attention\\ops\\blocksparse_attention\\interface.py -> build\\lib\\vllm\\attention\\ops\\blocksparse_attention\n",
            "  copying vllm\\attention\\ops\\blocksparse_attention\\utils.py -> build\\lib\\vllm\\attention\\ops\\blocksparse_attention\n",
            "  copying vllm\\attention\\ops\\blocksparse_attention\\__init__.py -> build\\lib\\vllm\\attention\\ops\\blocksparse_attention\n",
            "  creating build\\lib\\vllm\\core\\block\n",
            "  copying vllm\\core\\block\\block_table.py -> build\\lib\\vllm\\core\\block\n",
            "  copying vllm\\core\\block\\common.py -> build\\lib\\vllm\\core\\block\n",
            "  copying vllm\\core\\block\\cpu_gpu_block_allocator.py -> build\\lib\\vllm\\core\\block\n",
            "  copying vllm\\core\\block\\interfaces.py -> build\\lib\\vllm\\core\\block\n",
            "  copying vllm\\core\\block\\naive_block.py -> build\\lib\\vllm\\core\\block\n",
            "  copying vllm\\core\\block\\prefix_caching_block.py -> build\\lib\\vllm\\core\\block\n",
            "  copying vllm\\core\\block\\utils.py -> build\\lib\\vllm\\core\\block\n",
            "  copying vllm\\core\\block\\__init__.py -> build\\lib\\vllm\\core\\block\n",
            "  creating build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\base_device_communicator.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\cpu_communicator.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\cuda_communicator.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\cuda_wrapper.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\custom_all_reduce.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\custom_all_reduce_utils.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\hpu_communicator.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\pynccl.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\pynccl_wrapper.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\shm_broadcast.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\tpu_communicator.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\xpu_communicator.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  copying vllm\\distributed\\device_communicators\\__init__.py -> build\\lib\\vllm\\distributed\\device_communicators\n",
            "  creating build\\lib\\vllm\\distributed\\kv_transfer\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_transfer_agent.py -> build\\lib\\vllm\\distributed\\kv_transfer\n",
            "  copying vllm\\distributed\\kv_transfer\\__init__.py -> build\\lib\\vllm\\distributed\\kv_transfer\n",
            "  creating build\\lib\\vllm\\distributed\\kv_transfer\\kv_connector\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_connector\\base.py -> build\\lib\\vllm\\distributed\\kv_transfer\\kv_connector\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_connector\\factory.py -> build\\lib\\vllm\\distributed\\kv_transfer\\kv_connector\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_connector\\simple_connector.py -> build\\lib\\vllm\\distributed\\kv_transfer\\kv_connector\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_connector\\__init__.py -> build\\lib\\vllm\\distributed\\kv_transfer\\kv_connector\n",
            "  creating build\\lib\\vllm\\distributed\\kv_transfer\\kv_lookup_buffer\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_lookup_buffer\\base.py -> build\\lib\\vllm\\distributed\\kv_transfer\\kv_lookup_buffer\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_lookup_buffer\\simple_buffer.py -> build\\lib\\vllm\\distributed\\kv_transfer\\kv_lookup_buffer\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_lookup_buffer\\__init__.py -> build\\lib\\vllm\\distributed\\kv_transfer\\kv_lookup_buffer\n",
            "  creating build\\lib\\vllm\\distributed\\kv_transfer\\kv_pipe\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_pipe\\base.py -> build\\lib\\vllm\\distributed\\kv_transfer\\kv_pipe\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_pipe\\mooncake_pipe.py -> build\\lib\\vllm\\distributed\\kv_transfer\\kv_pipe\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_pipe\\pynccl_pipe.py -> build\\lib\\vllm\\distributed\\kv_transfer\\kv_pipe\n",
            "  copying vllm\\distributed\\kv_transfer\\kv_pipe\\__init__.py -> build\\lib\\vllm\\distributed\\kv_transfer\\kv_pipe\n",
            "  creating build\\lib\\vllm\\engine\\multiprocessing\n",
            "  copying vllm\\engine\\multiprocessing\\client.py -> build\\lib\\vllm\\engine\\multiprocessing\n",
            "  copying vllm\\engine\\multiprocessing\\engine.py -> build\\lib\\vllm\\engine\\multiprocessing\n",
            "  copying vllm\\engine\\multiprocessing\\__init__.py -> build\\lib\\vllm\\engine\\multiprocessing\n",
            "  creating build\\lib\\vllm\\engine\\output_processor\n",
            "  copying vllm\\engine\\output_processor\\interfaces.py -> build\\lib\\vllm\\engine\\output_processor\n",
            "  copying vllm\\engine\\output_processor\\multi_step.py -> build\\lib\\vllm\\engine\\output_processor\n",
            "  copying vllm\\engine\\output_processor\\single_step.py -> build\\lib\\vllm\\engine\\output_processor\n",
            "  copying vllm\\engine\\output_processor\\stop_checker.py -> build\\lib\\vllm\\engine\\output_processor\n",
            "  copying vllm\\engine\\output_processor\\util.py -> build\\lib\\vllm\\engine\\output_processor\n",
            "  copying vllm\\engine\\output_processor\\__init__.py -> build\\lib\\vllm\\engine\\output_processor\n",
            "  creating build\\lib\\vllm\\entrypoints\\cli\n",
            "  copying vllm\\entrypoints\\cli\\main.py -> build\\lib\\vllm\\entrypoints\\cli\n",
            "  copying vllm\\entrypoints\\cli\\openai.py -> build\\lib\\vllm\\entrypoints\\cli\n",
            "  copying vllm\\entrypoints\\cli\\serve.py -> build\\lib\\vllm\\entrypoints\\cli\n",
            "  copying vllm\\entrypoints\\cli\\types.py -> build\\lib\\vllm\\entrypoints\\cli\n",
            "  copying vllm\\entrypoints\\cli\\__init__.py -> build\\lib\\vllm\\entrypoints\\cli\n",
            "  creating build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\api_server.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\cli_args.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\logits_processors.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\protocol.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\run_batch.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\serving_chat.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\serving_completion.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\serving_embedding.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\serving_engine.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\serving_models.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\serving_pooling.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\serving_rerank.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\serving_score.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\serving_tokenization.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\serving_transcription.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  copying vllm\\entrypoints\\openai\\__init__.py -> build\\lib\\vllm\\entrypoints\\openai\n",
            "  creating build\\lib\\vllm\\entrypoints\\openai\\reasoning_parsers\n",
            "  copying vllm\\entrypoints\\openai\\reasoning_parsers\\abs_reasoning_parsers.py -> build\\lib\\vllm\\entrypoints\\openai\\reasoning_parsers\n",
            "  copying vllm\\entrypoints\\openai\\reasoning_parsers\\deepseek_r1_reasoning_parser.py -> build\\lib\\vllm\\entrypoints\\openai\\reasoning_parsers\n",
            "  copying vllm\\entrypoints\\openai\\reasoning_parsers\\__init__.py -> build\\lib\\vllm\\entrypoints\\openai\\reasoning_parsers\n",
            "  creating build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying vllm\\entrypoints\\openai\\tool_parsers\\abstract_tool_parser.py -> build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying vllm\\entrypoints\\openai\\tool_parsers\\granite_20b_fc_tool_parser.py -> build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying vllm\\entrypoints\\openai\\tool_parsers\\granite_tool_parser.py -> build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying vllm\\entrypoints\\openai\\tool_parsers\\hermes_tool_parser.py -> build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying vllm\\entrypoints\\openai\\tool_parsers\\internlm2_tool_parser.py -> build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying vllm\\entrypoints\\openai\\tool_parsers\\jamba_tool_parser.py -> build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying vllm\\entrypoints\\openai\\tool_parsers\\llama_tool_parser.py -> build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying vllm\\entrypoints\\openai\\tool_parsers\\mistral_tool_parser.py -> build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying vllm\\entrypoints\\openai\\tool_parsers\\pythonic_tool_parser.py -> build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying vllm\\entrypoints\\openai\\tool_parsers\\utils.py -> build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying vllm\\entrypoints\\openai\\tool_parsers\\__init__.py -> build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  creating build\\lib\\vllm\\lora\\ops\n",
            "  copying vllm\\lora\\ops\\__init__.py -> build\\lib\\vllm\\lora\\ops\n",
            "  creating build\\lib\\vllm\\lora\\punica_wrapper\n",
            "  copying vllm\\lora\\punica_wrapper\\punica_base.py -> build\\lib\\vllm\\lora\\punica_wrapper\n",
            "  copying vllm\\lora\\punica_wrapper\\punica_cpu.py -> build\\lib\\vllm\\lora\\punica_wrapper\n",
            "  copying vllm\\lora\\punica_wrapper\\punica_gpu.py -> build\\lib\\vllm\\lora\\punica_wrapper\n",
            "  copying vllm\\lora\\punica_wrapper\\punica_hpu.py -> build\\lib\\vllm\\lora\\punica_wrapper\n",
            "  copying vllm\\lora\\punica_wrapper\\punica_selector.py -> build\\lib\\vllm\\lora\\punica_wrapper\n",
            "  copying vllm\\lora\\punica_wrapper\\utils.py -> build\\lib\\vllm\\lora\\punica_wrapper\n",
            "  copying vllm\\lora\\punica_wrapper\\__init__.py -> build\\lib\\vllm\\lora\\punica_wrapper\n",
            "  creating build\\lib\\vllm\\lora\\ops\\torch_ops\n",
            "  copying vllm\\lora\\ops\\torch_ops\\lora_ops.py -> build\\lib\\vllm\\lora\\ops\\torch_ops\n",
            "  copying vllm\\lora\\ops\\torch_ops\\__init__.py -> build\\lib\\vllm\\lora\\ops\\torch_ops\n",
            "  creating build\\lib\\vllm\\lora\\ops\\triton_ops\n",
            "  copying vllm\\lora\\ops\\triton_ops\\bgmv_expand.py -> build\\lib\\vllm\\lora\\ops\\triton_ops\n",
            "  copying vllm\\lora\\ops\\triton_ops\\bgmv_expand_slice.py -> build\\lib\\vllm\\lora\\ops\\triton_ops\n",
            "  copying vllm\\lora\\ops\\triton_ops\\bgmv_shrink.py -> build\\lib\\vllm\\lora\\ops\\triton_ops\n",
            "  copying vllm\\lora\\ops\\triton_ops\\kernel_utils.py -> build\\lib\\vllm\\lora\\ops\\triton_ops\n",
            "  copying vllm\\lora\\ops\\triton_ops\\sgmv_expand.py -> build\\lib\\vllm\\lora\\ops\\triton_ops\n",
            "  copying vllm\\lora\\ops\\triton_ops\\sgmv_shrink.py -> build\\lib\\vllm\\lora\\ops\\triton_ops\n",
            "  copying vllm\\lora\\ops\\triton_ops\\utils.py -> build\\lib\\vllm\\lora\\ops\\triton_ops\n",
            "  copying vllm\\lora\\ops\\triton_ops\\__init__.py -> build\\lib\\vllm\\lora\\ops\\triton_ops\n",
            "  creating build\\lib\\vllm\\model_executor\\guided_decoding\n",
            "  copying vllm\\model_executor\\guided_decoding\\guided_fields.py -> build\\lib\\vllm\\model_executor\\guided_decoding\n",
            "  copying vllm\\model_executor\\guided_decoding\\lm_format_enforcer_decoding.py -> build\\lib\\vllm\\model_executor\\guided_decoding\n",
            "  copying vllm\\model_executor\\guided_decoding\\outlines_decoding.py -> build\\lib\\vllm\\model_executor\\guided_decoding\n",
            "  copying vllm\\model_executor\\guided_decoding\\outlines_logits_processors.py -> build\\lib\\vllm\\model_executor\\guided_decoding\n",
            "  copying vllm\\model_executor\\guided_decoding\\utils.py -> build\\lib\\vllm\\model_executor\\guided_decoding\n",
            "  copying vllm\\model_executor\\guided_decoding\\xgrammar_decoding.py -> build\\lib\\vllm\\model_executor\\guided_decoding\n",
            "  copying vllm\\model_executor\\guided_decoding\\__init__.py -> build\\lib\\vllm\\model_executor\\guided_decoding\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\activation.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\layernorm.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\linear.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\logits_processor.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\pooler.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\rejection_sampler.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\resampler.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\rotary_embedding.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\sampler.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\spec_decode_base_sampler.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\typical_acceptance_sampler.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\utils.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\vocab_parallel_embedding.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  copying vllm\\model_executor\\layers\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\n",
            "  creating build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\adapters.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\arctic.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\aria.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\baichuan.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\bamba.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\bart.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\bert.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\blip.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\blip2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\bloom.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\chameleon.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\chatglm.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\clip.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\commandr.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\dbrx.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\decilm.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\deepseek.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\deepseek_mtp.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\deepseek_v2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\deepseek_vl2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\eagle.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\exaone.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\fairseq2_llama.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\falcon.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\florence2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\fuyu.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\gemma.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\gemma2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\glm.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\glm4v.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\gpt2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\gpt_bigcode.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\gpt_j.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\gpt_neox.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\granite.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\granitemoe.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\gritlm.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\h2ovl.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\idefics2_vision_model.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\idefics3.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\interfaces.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\interfaces_base.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\internlm2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\internlm2_ve.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\internvl.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\intern_vit.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\jais.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\jamba.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\llama.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\llava.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\llava_next.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\llava_next_video.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\llava_onevision.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\mamba.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\mamba2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\mamba_cache.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\medusa.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\minicpm.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\minicpm3.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\minicpmo.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\minicpmv.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\mixtral.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\mixtral_quant.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\mllama.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\mlp_speculator.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\module_mapping.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\molmo.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\mpt.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\nemotron.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\nvlm_d.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\olmo.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\olmo2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\olmoe.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\opt.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\orion.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\paligemma.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\persimmon.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\phi.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\phi3.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\phi3v.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\phi3_small.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\phimoe.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\pixtral.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\prithvi_geospatial_mae.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\qwen.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\qwen2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\qwen2_5_vl.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\qwen2_audio.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\qwen2_moe.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\qwen2_rm.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\qwen2_vl.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\qwen_vl.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\registry.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\roberta.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\siglip.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\solar.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\stablelm.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\starcoder2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\telechat2.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\transformers.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\ultravox.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\utils.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\vision.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\whisper.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  copying vllm\\model_executor\\models\\__init__.py -> build\\lib\\vllm\\model_executor\\models\n",
            "  creating build\\lib\\vllm\\model_executor\\model_loader\n",
            "  copying vllm\\model_executor\\model_loader\\loader.py -> build\\lib\\vllm\\model_executor\\model_loader\n",
            "  copying vllm\\model_executor\\model_loader\\neuron.py -> build\\lib\\vllm\\model_executor\\model_loader\n",
            "  copying vllm\\model_executor\\model_loader\\openvino.py -> build\\lib\\vllm\\model_executor\\model_loader\n",
            "  copying vllm\\model_executor\\model_loader\\tensorizer.py -> build\\lib\\vllm\\model_executor\\model_loader\n",
            "  copying vllm\\model_executor\\model_loader\\utils.py -> build\\lib\\vllm\\model_executor\\model_loader\n",
            "  copying vllm\\model_executor\\model_loader\\weight_utils.py -> build\\lib\\vllm\\model_executor\\model_loader\n",
            "  copying vllm\\model_executor\\model_loader\\__init__.py -> build\\lib\\vllm\\model_executor\\model_loader\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\fused_marlin_moe.py -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\fused_moe.py -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\layer.py -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\moe_pallas.py -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\moe_torch_iterative.py -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\mamba\n",
            "  copying vllm\\model_executor\\layers\\mamba\\mamba_mixer.py -> build\\lib\\vllm\\model_executor\\layers\\mamba\n",
            "  copying vllm\\model_executor\\layers\\mamba\\mamba_mixer2.py -> build\\lib\\vllm\\model_executor\\layers\\mamba\n",
            "  copying vllm\\model_executor\\layers\\mamba\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\mamba\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\aqlm.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\awq.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\awq_marlin.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\awq_triton.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\base_config.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\bitsandbytes.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\deepspeedfp.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\experts_int8.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\fbgemm_fp8.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\fp8.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\gguf.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\gptq.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\gptq_marlin.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\gptq_marlin_24.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\hqq_marlin.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\ipex_quant.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kv_cache.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\marlin.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\modelopt.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\moe_wna16.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\neuron_quant.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\ptpc_fp8.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\qqq.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\schema.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\tpu_int8.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  copying vllm\\model_executor\\layers\\quantization\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying vllm\\model_executor\\layers\\mamba\\ops\\causal_conv1d.py -> build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying vllm\\model_executor\\layers\\mamba\\ops\\mamba_ssm.py -> build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying vllm\\model_executor\\layers\\mamba\\ops\\ssd_bmm.py -> build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying vllm\\model_executor\\layers\\mamba\\ops\\ssd_chunk_scan.py -> build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying vllm\\model_executor\\layers\\mamba\\ops\\ssd_chunk_state.py -> build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying vllm\\model_executor\\layers\\mamba\\ops\\ssd_combined.py -> build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying vllm\\model_executor\\layers\\mamba\\ops\\ssd_state_passing.py -> build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying vllm\\model_executor\\layers\\mamba\\ops\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\compressed_tensors.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\compressed_tensors_moe.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\triton_scaled_mm.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\utils.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kernels\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\n",
            "  copying vllm\\model_executor\\layers\\quantization\\quark\\quark.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\n",
            "  copying vllm\\model_executor\\layers\\quantization\\quark\\quark_moe.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\n",
            "  copying vllm\\model_executor\\layers\\quantization\\quark\\utils.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\n",
            "  copying vllm\\model_executor\\layers\\quantization\\quark\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\fp8_utils.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\gptq_utils.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\layer_utils.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\machete_utils.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\marlin_utils.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\marlin_utils_fp8.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\marlin_utils_test.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\marlin_utils_test_24.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\marlin_utils_test_qqq.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\quant_utils.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\w8a8_utils.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_24.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_scheme.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_w4a16_24.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_w8a16_fp8.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_w8a8_fp8.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_w8a8_int8.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_wNa16.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\\exllama.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\\machete.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\\marlin.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\\MPLinearKernel.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\\cutlass.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\\ScaledMMLinearKernel.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\\triton.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\\xla.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  copying vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\quark\\schemes\\quark_scheme.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\quark\\schemes\\quark_w8a8_fp8.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\quark\\schemes\\quark_w8a8_int8.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\n",
            "  copying vllm\\model_executor\\layers\\quantization\\quark\\schemes\\__init__.py -> build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\n",
            "  creating build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\arctic.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\chatglm.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\cohere2.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\dbrx.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\deepseek_vl2.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\eagle.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\exaone.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\falcon.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\h2ovl.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\internvl.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\jais.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\medusa.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\mllama.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\mlp_speculator.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\mpt.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\nemotron.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\nvlm_d.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\olmo2.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\solar.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\telechat2.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\ultravox.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  copying vllm\\transformers_utils\\configs\\__init__.py -> build\\lib\\vllm\\transformers_utils\\configs\n",
            "  creating build\\lib\\vllm\\transformers_utils\\processors\n",
            "  copying vllm\\transformers_utils\\processors\\deepseek_vl2.py -> build\\lib\\vllm\\transformers_utils\\processors\n",
            "  copying vllm\\transformers_utils\\processors\\__init__.py -> build\\lib\\vllm\\transformers_utils\\processors\n",
            "  creating build\\lib\\vllm\\transformers_utils\\tokenizers\n",
            "  copying vllm\\transformers_utils\\tokenizers\\mistral.py -> build\\lib\\vllm\\transformers_utils\\tokenizers\n",
            "  copying vllm\\transformers_utils\\tokenizers\\__init__.py -> build\\lib\\vllm\\transformers_utils\\tokenizers\n",
            "  creating build\\lib\\vllm\\transformers_utils\\tokenizer_group\n",
            "  copying vllm\\transformers_utils\\tokenizer_group\\base_tokenizer_group.py -> build\\lib\\vllm\\transformers_utils\\tokenizer_group\n",
            "  copying vllm\\transformers_utils\\tokenizer_group\\ray_tokenizer_group.py -> build\\lib\\vllm\\transformers_utils\\tokenizer_group\n",
            "  copying vllm\\transformers_utils\\tokenizer_group\\tokenizer_group.py -> build\\lib\\vllm\\transformers_utils\\tokenizer_group\n",
            "  copying vllm\\transformers_utils\\tokenizer_group\\__init__.py -> build\\lib\\vllm\\transformers_utils\\tokenizer_group\n",
            "  creating build\\lib\\vllm\\v1\\attention\n",
            "  copying vllm\\v1\\attention\\__init__.py -> build\\lib\\vllm\\v1\\attention\n",
            "  creating build\\lib\\vllm\\v1\\core\n",
            "  copying vllm\\v1\\core\\encoder_cache_manager.py -> build\\lib\\vllm\\v1\\core\n",
            "  copying vllm\\v1\\core\\kv_cache_manager.py -> build\\lib\\vllm\\v1\\core\n",
            "  copying vllm\\v1\\core\\kv_cache_utils.py -> build\\lib\\vllm\\v1\\core\n",
            "  copying vllm\\v1\\core\\scheduler.py -> build\\lib\\vllm\\v1\\core\n",
            "  copying vllm\\v1\\core\\scheduler_output.py -> build\\lib\\vllm\\v1\\core\n",
            "  copying vllm\\v1\\core\\__init__.py -> build\\lib\\vllm\\v1\\core\n",
            "  creating build\\lib\\vllm\\v1\\engine\n",
            "  copying vllm\\v1\\engine\\async_llm.py -> build\\lib\\vllm\\v1\\engine\n",
            "  copying vllm\\v1\\engine\\core.py -> build\\lib\\vllm\\v1\\engine\n",
            "  copying vllm\\v1\\engine\\core_client.py -> build\\lib\\vllm\\v1\\engine\n",
            "  copying vllm\\v1\\engine\\detokenizer.py -> build\\lib\\vllm\\v1\\engine\n",
            "  copying vllm\\v1\\engine\\llm_engine.py -> build\\lib\\vllm\\v1\\engine\n",
            "  copying vllm\\v1\\engine\\logprobs.py -> build\\lib\\vllm\\v1\\engine\n",
            "  copying vllm\\v1\\engine\\mm_input_cache.py -> build\\lib\\vllm\\v1\\engine\n",
            "  copying vllm\\v1\\engine\\output_processor.py -> build\\lib\\vllm\\v1\\engine\n",
            "  copying vllm\\v1\\engine\\processor.py -> build\\lib\\vllm\\v1\\engine\n",
            "  copying vllm\\v1\\engine\\__init__.py -> build\\lib\\vllm\\v1\\engine\n",
            "  creating build\\lib\\vllm\\v1\\executor\n",
            "  copying vllm\\v1\\executor\\abstract.py -> build\\lib\\vllm\\v1\\executor\n",
            "  copying vllm\\v1\\executor\\multiproc_executor.py -> build\\lib\\vllm\\v1\\executor\n",
            "  copying vllm\\v1\\executor\\ray_distributed_executor.py -> build\\lib\\vllm\\v1\\executor\n",
            "  copying vllm\\v1\\executor\\__init__.py -> build\\lib\\vllm\\v1\\executor\n",
            "  creating build\\lib\\vllm\\v1\\metrics\n",
            "  copying vllm\\v1\\metrics\\loggers.py -> build\\lib\\vllm\\v1\\metrics\n",
            "  copying vllm\\v1\\metrics\\stats.py -> build\\lib\\vllm\\v1\\metrics\n",
            "  copying vllm\\v1\\metrics\\__init__.py -> build\\lib\\vllm\\v1\\metrics\n",
            "  creating build\\lib\\vllm\\v1\\sample\n",
            "  copying vllm\\v1\\sample\\metadata.py -> build\\lib\\vllm\\v1\\sample\n",
            "  copying vllm\\v1\\sample\\rejection_sampler.py -> build\\lib\\vllm\\v1\\sample\n",
            "  copying vllm\\v1\\sample\\sampler.py -> build\\lib\\vllm\\v1\\sample\n",
            "  copying vllm\\v1\\sample\\__init__.py -> build\\lib\\vllm\\v1\\sample\n",
            "  creating build\\lib\\vllm\\v1\\spec_decode\n",
            "  copying vllm\\v1\\spec_decode\\ngram_proposer.py -> build\\lib\\vllm\\v1\\spec_decode\n",
            "  copying vllm\\v1\\spec_decode\\__init__.py -> build\\lib\\vllm\\v1\\spec_decode\n",
            "  creating build\\lib\\vllm\\v1\\stats\n",
            "  copying vllm\\v1\\stats\\common.py -> build\\lib\\vllm\\v1\\stats\n",
            "  copying vllm\\v1\\stats\\__init__.py -> build\\lib\\vllm\\v1\\stats\n",
            "  creating build\\lib\\vllm\\v1\\worker\n",
            "  copying vllm\\v1\\worker\\block_table.py -> build\\lib\\vllm\\v1\\worker\n",
            "  copying vllm\\v1\\worker\\gpu_input_batch.py -> build\\lib\\vllm\\v1\\worker\n",
            "  copying vllm\\v1\\worker\\gpu_model_runner.py -> build\\lib\\vllm\\v1\\worker\n",
            "  copying vllm\\v1\\worker\\gpu_worker.py -> build\\lib\\vllm\\v1\\worker\n",
            "  copying vllm\\v1\\worker\\lora_model_runner_mixin.py -> build\\lib\\vllm\\v1\\worker\n",
            "  copying vllm\\v1\\worker\\tpu_model_runner.py -> build\\lib\\vllm\\v1\\worker\n",
            "  copying vllm\\v1\\worker\\tpu_worker.py -> build\\lib\\vllm\\v1\\worker\n",
            "  copying vllm\\v1\\worker\\worker_base.py -> build\\lib\\vllm\\v1\\worker\n",
            "  copying vllm\\v1\\worker\\__init__.py -> build\\lib\\vllm\\v1\\worker\n",
            "  creating build\\lib\\vllm\\v1\\attention\\backends\n",
            "  copying vllm\\v1\\attention\\backends\\flash_attn.py -> build\\lib\\vllm\\v1\\attention\\backends\n",
            "  copying vllm\\v1\\attention\\backends\\pallas.py -> build\\lib\\vllm\\v1\\attention\\backends\n",
            "  copying vllm\\v1\\attention\\backends\\rocm_attn.py -> build\\lib\\vllm\\v1\\attention\\backends\n",
            "  copying vllm\\v1\\attention\\backends\\__init__.py -> build\\lib\\vllm\\v1\\attention\\backends\n",
            "  creating build\\lib\\vllm\\v1\\sample\\ops\n",
            "  copying vllm\\v1\\sample\\ops\\penalties.py -> build\\lib\\vllm\\v1\\sample\\ops\n",
            "  copying vllm\\v1\\sample\\ops\\topk_topp_sampler.py -> build\\lib\\vllm\\v1\\sample\\ops\n",
            "  copying vllm\\v1\\sample\\ops\\__init__.py -> build\\lib\\vllm\\v1\\sample\\ops\n",
            "  running egg_info\n",
            "  writing vllm.egg-info\\PKG-INFO\n",
            "  writing dependency_links to vllm.egg-info\\dependency_links.txt\n",
            "  writing entry points to vllm.egg-info\\entry_points.txt\n",
            "  writing requirements to vllm.egg-info\\requires.txt\n",
            "  writing top-level names to vllm.egg-info\\top_level.txt\n",
            "  reading manifest file 'vllm.egg-info\\SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'vllm.egg-info\\SOURCES.txt'\n",
            "  C:\\Users\\Test\\AppData\\Local\\Temp\\pip-build-env-wdoo4r3e\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'vllm.model_executor.layers.fused_moe.configs' is absent from the `packages` configuration.\n",
            "  !!\n",
            "  \n",
            "          ********************************************************************************\n",
            "          ############################\n",
            "          # Package would be ignored #\n",
            "          ############################\n",
            "          Python recognizes 'vllm.model_executor.layers.fused_moe.configs' as an importable package[^1],\n",
            "          but it is absent from setuptools' `packages` configuration.\n",
            "  \n",
            "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "          package, please make sure that 'vllm.model_executor.layers.fused_moe.configs' is explicitly added\n",
            "          to the `packages` configuration field.\n",
            "  \n",
            "          Alternatively, you can also rely on setuptools' discovery methods\n",
            "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "          instead of `find_packages(...)`/`find:`).\n",
            "  \n",
            "          You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \n",
            "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \n",
            "          If you don't want 'vllm.model_executor.layers.fused_moe.configs' to be distributed and are\n",
            "          already explicitly excluding 'vllm.model_executor.layers.fused_moe.configs' via\n",
            "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "          combination with a more fine grained `package-data` configuration.\n",
            "  \n",
            "          You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \n",
            "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \n",
            "  \n",
            "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "                even if it does not contain any `.py` files.\n",
            "                On the other hand, currently there is no concept of package data\n",
            "                directory, all directories are treated like packages.\n",
            "          ********************************************************************************\n",
            "  \n",
            "  !!\n",
            "    check.warn(importable)\n",
            "  C:\\Users\\Test\\AppData\\Local\\Temp\\pip-build-env-wdoo4r3e\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'vllm.model_executor.layers.quantization.utils.configs' is absent from the `packages` configuration.\n",
            "  !!\n",
            "  \n",
            "          ********************************************************************************\n",
            "          ############################\n",
            "          # Package would be ignored #\n",
            "          ############################\n",
            "          Python recognizes 'vllm.model_executor.layers.quantization.utils.configs' as an importable package[^1],\n",
            "          but it is absent from setuptools' `packages` configuration.\n",
            "  \n",
            "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "          package, please make sure that 'vllm.model_executor.layers.quantization.utils.configs' is explicitly added\n",
            "          to the `packages` configuration field.\n",
            "  \n",
            "          Alternatively, you can also rely on setuptools' discovery methods\n",
            "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "          instead of `find_packages(...)`/`find:`).\n",
            "  \n",
            "          You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \n",
            "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \n",
            "          If you don't want 'vllm.model_executor.layers.quantization.utils.configs' to be distributed and are\n",
            "          already explicitly excluding 'vllm.model_executor.layers.quantization.utils.configs' via\n",
            "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "          combination with a more fine grained `package-data` configuration.\n",
            "  \n",
            "          You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \n",
            "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \n",
            "  \n",
            "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "                even if it does not contain any `.py` files.\n",
            "                On the other hand, currently there is no concept of package data\n",
            "                directory, all directories are treated like packages.\n",
            "          ********************************************************************************\n",
            "  \n",
            "  !!\n",
            "    check.warn(importable)\n",
            "  C:\\Users\\Test\\AppData\\Local\\Temp\\pip-build-env-wdoo4r3e\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'vllm.vllm_flash_attn' is absent from the `packages` configuration.\n",
            "  !!\n",
            "  \n",
            "          ********************************************************************************\n",
            "          ############################\n",
            "          # Package would be ignored #\n",
            "          ############################\n",
            "          Python recognizes 'vllm.vllm_flash_attn' as an importable package[^1],\n",
            "          but it is absent from setuptools' `packages` configuration.\n",
            "  \n",
            "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "          package, please make sure that 'vllm.vllm_flash_attn' is explicitly added\n",
            "          to the `packages` configuration field.\n",
            "  \n",
            "          Alternatively, you can also rely on setuptools' discovery methods\n",
            "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "          instead of `find_packages(...)`/`find:`).\n",
            "  \n",
            "          You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \n",
            "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \n",
            "          If you don't want 'vllm.vllm_flash_attn' to be distributed and are\n",
            "          already explicitly excluding 'vllm.vllm_flash_attn' via\n",
            "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "          combination with a more fine grained `package-data` configuration.\n",
            "  \n",
            "          You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \n",
            "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \n",
            "  \n",
            "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "                even if it does not contain any `.py` files.\n",
            "                On the other hand, currently there is no concept of package data\n",
            "                directory, all directories are treated like packages.\n",
            "          ********************************************************************************\n",
            "  \n",
            "  !!\n",
            "    check.warn(importable)\n",
            "  copying vllm\\py.typed -> build\\lib\\vllm\n",
            "  creating build\\lib\\vllm\\vllm_flash_attn\n",
            "  copying vllm\\vllm_flash_attn\\.gitkeep -> build\\lib\\vllm\\vllm_flash_attn\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=3072,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=1344,device_name=NVIDIA_A100-SXM4-40GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=1344,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=1344,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=14336,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=14336,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=1792,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=2688,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=2688,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=3072,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=3200,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=3584,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=6400,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=800,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=256,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=60,N=1408,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=60,N=176,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=60,N=352,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=60,N=704,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=64,N=1280,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=64,N=1280,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=64,N=640,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=64,N=640,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=14336,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=14336,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=14336,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=14336,device_name=AMD_Instinct_MI325X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=14336,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=16384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=16384,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=16384,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=16384,device_name=AMD_Instinct_MI325X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=AMD_Instinct_MI325X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=NVIDIA_A100-SXM4-40GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=AMD_Instinct_MI325X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=AMD_Instinct_MI325X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=NVIDIA_A100-SXM4-40GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=NVIDIA_L40S.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=AMD_Instinct_MI325X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=AMD_Instinct_MI325X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=8192,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=8192,device_name=AMD_Instinct_MI300X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=8192,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=8192,device_name=AMD_Instinct_MI325X.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=8192,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  creating build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=1536,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=1536,K=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=1536,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=1536,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=2048,K=512,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=2048,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=2048,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=2304,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=2304,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=2304,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=24576,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=24576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=24576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=24576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=256,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=256,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=3072,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=3072,K=1536,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=3072,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=3072,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=3072,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=3072,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=32768,K=512,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=32768,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=32768,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=32768,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=36864,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=36864,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=36864,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=4096,K=512,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=4096,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=4096,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=4608,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=4608,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=4608,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=512,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=512,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=512,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=576,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=1024,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=1024,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=1152,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=1152,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=1152,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=128,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=16384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=16384,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=16384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=16384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=18432,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=18432,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=18432,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=18432,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=2048,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=2304,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=2304,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=2304,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=256,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=7168,K=8192,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=8192,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying vllm\\distributed\\kv_transfer\\README.md -> build\\lib\\vllm\\distributed\\kv_transfer\n",
            "  copying vllm\\distributed\\kv_transfer\\disagg_prefill_workflow.jpg -> build\\lib\\vllm\\distributed\\kv_transfer\n",
            "  copying vllm\\model_executor\\layers\\fused_moe\\configs\\README -> build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  installing to build\\bdist.win-amd64\\wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build\\bdist.win-amd64\\wheel\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\adapter_commons\n",
            "  copying build\\lib\\vllm\\adapter_commons\\layers.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\adapter_commons\n",
            "  copying build\\lib\\vllm\\adapter_commons\\models.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\adapter_commons\n",
            "  copying build\\lib\\vllm\\adapter_commons\\request.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\adapter_commons\n",
            "  copying build\\lib\\vllm\\adapter_commons\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\adapter_commons\n",
            "  copying build\\lib\\vllm\\adapter_commons\\worker_manager.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\adapter_commons\n",
            "  copying build\\lib\\vllm\\adapter_commons\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\adapter_commons\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\assets\n",
            "  copying build\\lib\\vllm\\assets\\audio.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\assets\n",
            "  copying build\\lib\\vllm\\assets\\base.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\assets\n",
            "  copying build\\lib\\vllm\\assets\\image.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\assets\n",
            "  copying build\\lib\\vllm\\assets\\video.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\assets\n",
            "  copying build\\lib\\vllm\\assets\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\assets\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\attention\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\abstract.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\blocksparse_attn.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\flashinfer.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\flash_attn.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\hpu_attn.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\ipex_attn.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\attention\\backends\\mla\n",
            "  copying build\\lib\\vllm\\attention\\backends\\mla\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\\mla\n",
            "  copying build\\lib\\vllm\\attention\\backends\\mla\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\\mla\n",
            "  copying build\\lib\\vllm\\attention\\backends\\openvino.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\pallas.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\placeholder_attn.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\rocm_flash_attn.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\torch_sdpa.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\triton_mla.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\xformers.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\backends\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\backends\n",
            "  copying build\\lib\\vllm\\attention\\layer.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\attention\\ops\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\attention\\ops\\blocksparse_attention\n",
            "  copying build\\lib\\vllm\\attention\\ops\\blocksparse_attention\\blocksparse_attention_kernel.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\\blocksparse_attention\n",
            "  copying build\\lib\\vllm\\attention\\ops\\blocksparse_attention\\interface.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\\blocksparse_attention\n",
            "  copying build\\lib\\vllm\\attention\\ops\\blocksparse_attention\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\\blocksparse_attention\n",
            "  copying build\\lib\\vllm\\attention\\ops\\blocksparse_attention\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\\blocksparse_attention\n",
            "  copying build\\lib\\vllm\\attention\\ops\\hpu_paged_attn.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\n",
            "  copying build\\lib\\vllm\\attention\\ops\\ipex_attn.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\n",
            "  copying build\\lib\\vllm\\attention\\ops\\nki_flash_attn.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\n",
            "  copying build\\lib\\vllm\\attention\\ops\\paged_attn.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\n",
            "  copying build\\lib\\vllm\\attention\\ops\\prefix_prefill.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\n",
            "  copying build\\lib\\vllm\\attention\\ops\\triton_decode_attention.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\n",
            "  copying build\\lib\\vllm\\attention\\ops\\triton_flash_attention.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\n",
            "  copying build\\lib\\vllm\\attention\\ops\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\\ops\n",
            "  copying build\\lib\\vllm\\attention\\selector.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\n",
            "  copying build\\lib\\vllm\\attention\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\attention\n",
            "  copying build\\lib\\vllm\\beam_search.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\backends.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\compiler_interface.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\counter.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\decorators.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\fix_functionalization.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\fusion.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\fx_utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\inductor_pass.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\monitor.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\multi_output_match.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\pass_manager.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\reshapes.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\vllm_inductor_pass.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\wrapper.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\compilation\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\compilation\n",
            "  copying build\\lib\\vllm\\config.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\n",
            "  copying build\\lib\\vllm\\connections.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\core\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\core\\block\n",
            "  copying build\\lib\\vllm\\core\\block\\block_table.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\\block\n",
            "  copying build\\lib\\vllm\\core\\block\\common.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\\block\n",
            "  copying build\\lib\\vllm\\core\\block\\cpu_gpu_block_allocator.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\\block\n",
            "  copying build\\lib\\vllm\\core\\block\\interfaces.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\\block\n",
            "  copying build\\lib\\vllm\\core\\block\\naive_block.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\\block\n",
            "  copying build\\lib\\vllm\\core\\block\\prefix_caching_block.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\\block\n",
            "  copying build\\lib\\vllm\\core\\block\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\\block\n",
            "  copying build\\lib\\vllm\\core\\block\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\\block\n",
            "  copying build\\lib\\vllm\\core\\block_manager.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\n",
            "  copying build\\lib\\vllm\\core\\evictor.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\n",
            "  copying build\\lib\\vllm\\core\\interfaces.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\n",
            "  copying build\\lib\\vllm\\core\\placeholder_block_space_manager.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\n",
            "  copying build\\lib\\vllm\\core\\scheduler.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\n",
            "  copying build\\lib\\vllm\\core\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\core\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\device_allocator\n",
            "  copying build\\lib\\vllm\\device_allocator\\cumem.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\device_allocator\n",
            "  copying build\\lib\\vllm\\device_allocator\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\device_allocator\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\distributed\n",
            "  copying build\\lib\\vllm\\distributed\\communication_op.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\base_device_communicator.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\cpu_communicator.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\cuda_communicator.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\cuda_wrapper.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\custom_all_reduce.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\custom_all_reduce_utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\hpu_communicator.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\pynccl.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\pynccl_wrapper.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\shm_broadcast.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\tpu_communicator.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\xpu_communicator.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  copying build\\lib\\vllm\\distributed\\device_communicators\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\device_communicators\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\distributed\\kv_transfer\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\disagg_prefill_workflow.jpg -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\distributed\\kv_transfer\\kv_connector\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_connector\\base.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\\kv_connector\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_connector\\factory.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\\kv_connector\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_connector\\simple_connector.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\\kv_connector\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_connector\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\\kv_connector\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\distributed\\kv_transfer\\kv_lookup_buffer\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_lookup_buffer\\base.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\\kv_lookup_buffer\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_lookup_buffer\\simple_buffer.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\\kv_lookup_buffer\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_lookup_buffer\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\\kv_lookup_buffer\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\distributed\\kv_transfer\\kv_pipe\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_pipe\\base.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\\kv_pipe\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_pipe\\mooncake_pipe.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\\kv_pipe\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_pipe\\pynccl_pipe.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\\kv_pipe\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_pipe\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\\kv_pipe\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\kv_transfer_agent.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\README.md -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\n",
            "  copying build\\lib\\vllm\\distributed\\kv_transfer\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\\kv_transfer\n",
            "  copying build\\lib\\vllm\\distributed\\parallel_state.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\n",
            "  copying build\\lib\\vllm\\distributed\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\n",
            "  copying build\\lib\\vllm\\distributed\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\distributed\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\engine\n",
            "  copying build\\lib\\vllm\\engine\\arg_utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\n",
            "  copying build\\lib\\vllm\\engine\\async_llm_engine.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\n",
            "  copying build\\lib\\vllm\\engine\\async_timeout.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\n",
            "  copying build\\lib\\vllm\\engine\\llm_engine.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\n",
            "  copying build\\lib\\vllm\\engine\\metrics.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\n",
            "  copying build\\lib\\vllm\\engine\\metrics_types.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\engine\\multiprocessing\n",
            "  copying build\\lib\\vllm\\engine\\multiprocessing\\client.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\\multiprocessing\n",
            "  copying build\\lib\\vllm\\engine\\multiprocessing\\engine.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\\multiprocessing\n",
            "  copying build\\lib\\vllm\\engine\\multiprocessing\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\\multiprocessing\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\engine\\output_processor\n",
            "  copying build\\lib\\vllm\\engine\\output_processor\\interfaces.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\\output_processor\n",
            "  copying build\\lib\\vllm\\engine\\output_processor\\multi_step.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\\output_processor\n",
            "  copying build\\lib\\vllm\\engine\\output_processor\\single_step.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\\output_processor\n",
            "  copying build\\lib\\vllm\\engine\\output_processor\\stop_checker.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\\output_processor\n",
            "  copying build\\lib\\vllm\\engine\\output_processor\\util.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\\output_processor\n",
            "  copying build\\lib\\vllm\\engine\\output_processor\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\\output_processor\n",
            "  copying build\\lib\\vllm\\engine\\protocol.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\n",
            "  copying build\\lib\\vllm\\engine\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\engine\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\entrypoints\n",
            "  copying build\\lib\\vllm\\entrypoints\\api_server.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\n",
            "  copying build\\lib\\vllm\\entrypoints\\chat_utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\entrypoints\\cli\n",
            "  copying build\\lib\\vllm\\entrypoints\\cli\\main.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\cli\n",
            "  copying build\\lib\\vllm\\entrypoints\\cli\\openai.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\cli\n",
            "  copying build\\lib\\vllm\\entrypoints\\cli\\serve.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\cli\n",
            "  copying build\\lib\\vllm\\entrypoints\\cli\\types.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\cli\n",
            "  copying build\\lib\\vllm\\entrypoints\\cli\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\cli\n",
            "  copying build\\lib\\vllm\\entrypoints\\launcher.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\n",
            "  copying build\\lib\\vllm\\entrypoints\\llm.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\n",
            "  copying build\\lib\\vllm\\entrypoints\\logger.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\api_server.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\cli_args.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\logits_processors.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\protocol.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\entrypoints\\openai\\reasoning_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\reasoning_parsers\\abs_reasoning_parsers.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\reasoning_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\reasoning_parsers\\deepseek_r1_reasoning_parser.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\reasoning_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\reasoning_parsers\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\reasoning_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\run_batch.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\serving_chat.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\serving_completion.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\serving_embedding.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\serving_engine.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\serving_models.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\serving_pooling.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\serving_rerank.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\serving_score.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\serving_tokenization.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\serving_transcription.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\\abstract_tool_parser.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\\granite_20b_fc_tool_parser.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\\granite_tool_parser.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\\hermes_tool_parser.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\\internlm2_tool_parser.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\\jamba_tool_parser.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\\llama_tool_parser.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\\mistral_tool_parser.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\\pythonic_tool_parser.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\tool_parsers\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\\tool_parsers\n",
            "  copying build\\lib\\vllm\\entrypoints\\openai\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\\openai\n",
            "  copying build\\lib\\vllm\\entrypoints\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\n",
            "  copying build\\lib\\vllm\\entrypoints\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\entrypoints\n",
            "  copying build\\lib\\vllm\\envs.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\executor\n",
            "  copying build\\lib\\vllm\\executor\\executor_base.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\executor\n",
            "  copying build\\lib\\vllm\\executor\\mp_distributed_executor.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\executor\n",
            "  copying build\\lib\\vllm\\executor\\msgspec_utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\executor\n",
            "  copying build\\lib\\vllm\\executor\\multiproc_worker_utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\executor\n",
            "  copying build\\lib\\vllm\\executor\\ray_distributed_executor.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\executor\n",
            "  copying build\\lib\\vllm\\executor\\ray_utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\executor\n",
            "  copying build\\lib\\vllm\\executor\\uniproc_executor.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\executor\n",
            "  copying build\\lib\\vllm\\executor\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\executor\n",
            "  copying build\\lib\\vllm\\forward_context.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\inputs\n",
            "  copying build\\lib\\vllm\\inputs\\data.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\inputs\n",
            "  copying build\\lib\\vllm\\inputs\\parse.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\inputs\n",
            "  copying build\\lib\\vllm\\inputs\\preprocess.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\inputs\n",
            "  copying build\\lib\\vllm\\inputs\\registry.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\inputs\n",
            "  copying build\\lib\\vllm\\inputs\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\inputs\n",
            "  copying build\\lib\\vllm\\logger.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\logging_utils\n",
            "  copying build\\lib\\vllm\\logging_utils\\formatter.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\logging_utils\n",
            "  copying build\\lib\\vllm\\logging_utils\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\logging_utils\n",
            "  copying build\\lib\\vllm\\logits_process.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\lora\n",
            "  copying build\\lib\\vllm\\lora\\fully_sharded_layers.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\n",
            "  copying build\\lib\\vllm\\lora\\layers.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\n",
            "  copying build\\lib\\vllm\\lora\\lora.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\n",
            "  copying build\\lib\\vllm\\lora\\models.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\lora\\ops\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\lora\\ops\\torch_ops\n",
            "  copying build\\lib\\vllm\\lora\\ops\\torch_ops\\lora_ops.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\ops\\torch_ops\n",
            "  copying build\\lib\\vllm\\lora\\ops\\torch_ops\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\ops\\torch_ops\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\lora\\ops\\triton_ops\n",
            "  copying build\\lib\\vllm\\lora\\ops\\triton_ops\\bgmv_expand.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\ops\\triton_ops\n",
            "  copying build\\lib\\vllm\\lora\\ops\\triton_ops\\bgmv_expand_slice.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\ops\\triton_ops\n",
            "  copying build\\lib\\vllm\\lora\\ops\\triton_ops\\bgmv_shrink.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\ops\\triton_ops\n",
            "  copying build\\lib\\vllm\\lora\\ops\\triton_ops\\kernel_utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\ops\\triton_ops\n",
            "  copying build\\lib\\vllm\\lora\\ops\\triton_ops\\sgmv_expand.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\ops\\triton_ops\n",
            "  copying build\\lib\\vllm\\lora\\ops\\triton_ops\\sgmv_shrink.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\ops\\triton_ops\n",
            "  copying build\\lib\\vllm\\lora\\ops\\triton_ops\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\ops\\triton_ops\n",
            "  copying build\\lib\\vllm\\lora\\ops\\triton_ops\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\ops\\triton_ops\n",
            "  copying build\\lib\\vllm\\lora\\ops\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\ops\n",
            "  copying build\\lib\\vllm\\lora\\peft_helper.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\lora\\punica_wrapper\n",
            "  copying build\\lib\\vllm\\lora\\punica_wrapper\\punica_base.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\punica_wrapper\n",
            "  copying build\\lib\\vllm\\lora\\punica_wrapper\\punica_cpu.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\punica_wrapper\n",
            "  copying build\\lib\\vllm\\lora\\punica_wrapper\\punica_gpu.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\punica_wrapper\n",
            "  copying build\\lib\\vllm\\lora\\punica_wrapper\\punica_hpu.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\punica_wrapper\n",
            "  copying build\\lib\\vllm\\lora\\punica_wrapper\\punica_selector.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\punica_wrapper\n",
            "  copying build\\lib\\vllm\\lora\\punica_wrapper\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\punica_wrapper\n",
            "  copying build\\lib\\vllm\\lora\\punica_wrapper\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\\punica_wrapper\n",
            "  copying build\\lib\\vllm\\lora\\request.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\n",
            "  copying build\\lib\\vllm\\lora\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\n",
            "  copying build\\lib\\vllm\\lora\\worker_manager.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\n",
            "  copying build\\lib\\vllm\\lora\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\lora\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\n",
            "  copying build\\lib\\vllm\\model_executor\\custom_op.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\guided_decoding\n",
            "  copying build\\lib\\vllm\\model_executor\\guided_decoding\\guided_fields.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\guided_decoding\n",
            "  copying build\\lib\\vllm\\model_executor\\guided_decoding\\lm_format_enforcer_decoding.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\guided_decoding\n",
            "  copying build\\lib\\vllm\\model_executor\\guided_decoding\\outlines_decoding.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\guided_decoding\n",
            "  copying build\\lib\\vllm\\model_executor\\guided_decoding\\outlines_logits_processors.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\guided_decoding\n",
            "  copying build\\lib\\vllm\\model_executor\\guided_decoding\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\guided_decoding\n",
            "  copying build\\lib\\vllm\\model_executor\\guided_decoding\\xgrammar_decoding.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\guided_decoding\n",
            "  copying build\\lib\\vllm\\model_executor\\guided_decoding\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\guided_decoding\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\activation.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\fused_moe\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=3072,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=1344,device_name=NVIDIA_A100-SXM4-40GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=1344,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=1344,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=14336,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=14336,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=1792,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=2688,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=2688,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=3072,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=3200,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=3584,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=6400,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=16,N=800,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=256,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=60,N=1408,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=60,N=176,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=60,N=352,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=60,N=704,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=64,N=1280,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=64,N=1280,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=64,N=640,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=64,N=640,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=14336,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=14336,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=14336,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=14336,device_name=AMD_Instinct_MI325X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=14336,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=16384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=16384,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=16384,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=16384,device_name=AMD_Instinct_MI325X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=AMD_Instinct_MI325X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=NVIDIA_A100-SXM4-40GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=1792,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=AMD_Instinct_MI325X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=AMD_Instinct_MI325X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=NVIDIA_A100-SXM4-40GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=3584,device_name=NVIDIA_L40S.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=AMD_Instinct_MI325X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=AMD_Instinct_MI325X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=8192,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=8192,device_name=AMD_Instinct_MI300X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=8192,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=8192,device_name=AMD_Instinct_MI325X.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\E=8,N=8192,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\configs\\README -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\fused_marlin_moe.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\fused_moe.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\layer.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\moe_pallas.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\moe_torch_iterative.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\fused_moe\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\fused_moe\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\layernorm.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\linear.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\logits_processor.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\mamba\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\mamba\\mamba_mixer.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\mamba\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\mamba\\mamba_mixer2.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\mamba\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\\causal_conv1d.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\\mamba_ssm.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\\ssd_bmm.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\\ssd_chunk_scan.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\\ssd_chunk_state.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\\ssd_combined.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\\ssd_state_passing.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\mamba\\ops\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\mamba\\ops\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\mamba\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\mamba\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\pooler.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\aqlm.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\awq.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\awq_marlin.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\awq_triton.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\base_config.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\bitsandbytes.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\compressed_tensors.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\compressed_tensors_moe.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_24.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_scheme.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_w4a16_24.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_w8a16_fp8.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_w8a8_fp8.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_w8a8_int8.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\compressed_tensors_wNa16.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\triton_scaled_mm.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\compressed_tensors\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\deepspeedfp.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\experts_int8.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\fbgemm_fp8.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\fp8.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\gguf.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\gptq.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\gptq_marlin.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\gptq_marlin_24.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\hqq_marlin.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\ipex_quant.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\quantization\\kernels\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\\exllama.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\\machete.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\\marlin.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\\MPLinearKernel.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\kernels\\mixed_precision\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\\cutlass.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\\ScaledMMLinearKernel.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\\triton.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\\xla.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\kernels\\scaled_mm\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kernels\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\kernels\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\kv_cache.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\marlin.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\modelopt.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\moe_wna16.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\neuron_quant.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\ptpc_fp8.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\qqq.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\quantization\\quark\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\quark.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\quark\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\quark_moe.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\quark\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\\quark_scheme.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\\quark_w8a8_fp8.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\\quark_w8a8_int8.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\quark\\schemes\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\utils.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\quark\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\quark\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\quark\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\schema.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\tpu_int8.py -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\quantization\\utils\n",
            "  creating build\\bdist.win-amd64\\wheel\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  copying build\\lib\\vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=1536,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json -> build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\utils\\configs\n",
            "  error: could not create 'build\\bdist.win-amd64\\wheel\\.\\vllm\\model_executor\\layers\\quantization\\utils\\configs\\N=1536,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json': No such file or directory\n",
            "  [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for vllm\n",
            "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (vllm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTULEMXuAwoY"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1zyu9Ug2XEt"
      },
      "source": [
        "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59DIs5BMcvjN",
        "outputId": "37994e88-b0d2-4833-83db-7899eec2ab94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Test\\AppData\\Roaming\\Python\\Python312\\site-packages\\unsloth\\__init__.py:152: UserWarning: Unsloth: Running `ldconfig /usr/lib64-nvidia` to link CUDA.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Test\\AppData\\Roaming\\Python\\Python312\\site-packages\\unsloth\\__init__.py:186: UserWarning: Unsloth: CUDA is not linked properly.\n",
            "Try running `python -m bitsandbytes` then `python -m xformers.info`\n",
            "We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn't work.\n",
            "You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\n",
            "Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\n",
            "Unsloth will still run for now, but maybe it might crash - let's hope it works!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.6.0+cu118)\n",
            "    Python  3.12.8 (you have 3.12.7)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8-SLRUB2gwM"
      },
      "source": [
        "Load up `Llama 3.1 8B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "DkIvEkIIkEyB",
        "outputId": "f92b32f8-154d-466a-f5c3-efc1dae0a83d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Unsloth: Please install vLLM before enabling `fast_inference`!\nYou can do this in a terminal via `pip install vllm`",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[28], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;66;03m# Can increase for longer reasoning traces\u001b[39;00m\n\u001b[0;32m      4\u001b[0m lora_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;66;03m# Larger rank = smarter, but slower\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      7\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     max_seq_length \u001b[38;5;241m=\u001b[39m max_seq_length,\n\u001b[0;32m      9\u001b[0m     load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# False for LoRA 16bit\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     fast_inference \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Enable vLLM fast inference\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     max_lora_rank \u001b[38;5;241m=\u001b[39m lora_rank,\n\u001b[0;32m     12\u001b[0m     gpu_memory_utilization \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;66;03m# Reduce if out of memory\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mget_peft_model(\n\u001b[0;32m     16\u001b[0m     model,\n\u001b[0;32m     17\u001b[0m     r \u001b[38;5;241m=\u001b[39m lora_rank, \u001b[38;5;66;03m# Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3407\u001b[39m,\n\u001b[0;32m     25\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\unsloth\\models\\loader.py:93\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fast_inference:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvllm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Please install vLLM before enabling `fast_inference`!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[0;32m     95\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can do this in a terminal via `pip install vllm`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: Unsloth: Please install vLLM before enabling `fast_inference`!\nYou can do this in a terminal via `pip install vllm`"
          ]
        }
      ],
      "source": [
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "max_seq_length = 512 # Can increase for longer reasoning traces\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
        "\n",
        "\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXk993X6C2ZZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Load and prep dataset\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "def format_recipe_as_xml(name: str, ingredients: str, instructions: str, nutrition: str) -> str:\n",
        "    return (f\"<recipe>\\n\"\n",
        "            f\"<name>\\n{name}\\n</name>\\n\"\n",
        "            f\"<ingredients>\\n{ingredients}\\n</ingredients>\\n\"\n",
        "            f\"<instructions>\\n{instructions}\\n</instructions>\\n\"\n",
        "            f\"<nutrition>\\n{nutrition}\\n</nutrition>\\n\"\n",
        "            f\"</recipe>\")\n",
        "\n",
        "# 更新数据集加载函数以适应新的食谱数据集\n",
        "def get_recipes_data(split='train') -> Dataset:\n",
        "    data = load_dataset(\"AkashPS11/recipes_data_food.com\", split=split)\n",
        "\n",
        "    def format_ingredients(quantities, parts):\n",
        "        if quantities and parts:\n",
        "            quantities_list = quantities.split(',')\n",
        "            parts_list = parts.split(',')\n",
        "            return ', '.join([f\"{q.strip()} {p.strip()}\" for q, p in zip(quantities_list, parts_list)])\n",
        "        return \"No ingredients available\"\n",
        "\n",
        "    # 过滤掉缺失关键字段的记录\n",
        "    data = data.filter(lambda x: (\n",
        "        x['Name'] and\n",
        "        x['RecipeInstructions'] and\n",
        "        x['RecipeIngredientQuantities'] and\n",
        "        x['RecipeIngredientParts'] and\n",
        "        x.get('Calories') is not None and\n",
        "        x.get('FatContent') is not None and\n",
        "        x.get('SaturatedFatContent') is not None and\n",
        "        x.get('CholesterolContent') is not None and\n",
        "        x.get('SodiumContent') is not None and\n",
        "        x.get('CarbohydrateContent') is not None and\n",
        "        x.get('FiberContent') is not None and\n",
        "        x.get('SugarContent') is not None and\n",
        "        x.get('ProteinContent') is not None\n",
        "    ))\n",
        "\n",
        "    data = data.map(lambda x: {\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['Name']}\n",
        "        ],\n",
        "        'answer': XML_COT_FORMAT.format(\n",
        "            reasoning=f\"Recipe for {x['Name']}\",  # 可以根据需要调整reasoning部分\n",
        "            answer=format_recipe_as_xml(\n",
        "                name=x['Name'],\n",
        "                ingredients=format_ingredients(\n",
        "                    x.get('RecipeIngredientQuantities', ''),\n",
        "                    x.get('RecipeIngredientParts', '')\n",
        "                ),\n",
        "                instructions=x['RecipeInstructions'],\n",
        "                nutrition=(\n",
        "                    f\"Calories: {x['Calories']}, \"\n",
        "                    f\"Fat: {x['FatContent']}g, \"\n",
        "                    f\"Saturated Fat: {x['SaturatedFatContent']}g, \"\n",
        "                    f\"Cholesterol: {x['CholesterolContent']}mg, \"\n",
        "                    f\"Sodium: {x['SodiumContent']}mg, \"\n",
        "                    f\"Carbohydrates: {x['CarbohydrateContent']}g, \"\n",
        "                    f\"Fiber: {x['FiberContent']}g, \"\n",
        "                    f\"Sugar: {x['SugarContent']}g, \"\n",
        "                    f\"Protein: {x['ProteinContent']}g\"\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    })\n",
        "    return data\n",
        "\n",
        "# 使用新函数来获取数据集\n",
        "dataset = get_recipes_data()\n",
        "\n",
        "# 打印数据集中前几个示例以查看输入和输出\n",
        "for i in range(5):  # 打印前5个示例\n",
        "    print(f\"Example {i+1}\")\n",
        "    print(\"Prompt:\")\n",
        "    print(dataset[i]['prompt'])\n",
        "    print(\"Answer:\")\n",
        "    print(dataset[i]['answer'])\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "\n",
        "# Reward functions\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqkXK2D4d6p"
      },
      "outputs": [],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True, # use vLLM for fast inference!\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 6, # Decrease if out of memory\n",
        "    max_prompt_length = 256,\n",
        "    max_completion_length = 200,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 250,\n",
        "    save_steps = 250,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzOuSVCL_GA9"
      },
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf_OY5WMVOxF"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDgFfhFYIAS"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6WOnKZOAwoa"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}